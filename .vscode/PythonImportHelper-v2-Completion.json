[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "attrgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "attrgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Popen",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "PIPE",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "list2cmdline",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Popen",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "PIPE",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "list2cmdline",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "timeit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timeit",
        "description": "timeit",
        "detail": "timeit",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NotRequired",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypedDict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NotRequired",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypedDict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "semver",
        "description": "semver",
        "isExtraImport": true,
        "detail": "semver",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "semver",
        "description": "semver",
        "isExtraImport": true,
        "detail": "semver",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "pairwise",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "islice",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "termcolor",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "termcolor",
        "description": "termcolor",
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "math,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math.",
        "description": "math.",
        "detail": "math.",
        "documentation": {}
    },
    {
        "label": "dill",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dill",
        "description": "dill",
        "detail": "dill",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "StringIO",
        "description": "StringIO",
        "detail": "StringIO",
        "documentation": {}
    },
    {
        "label": "pands",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pands",
        "description": "pands",
        "detail": "pands",
        "documentation": {}
    },
    {
        "label": "Object",
        "importPath": "my_lib",
        "description": "my_lib",
        "isExtraImport": true,
        "detail": "my_lib",
        "documentation": {}
    },
    {
        "label": "Object3",
        "importPath": "my_lib",
        "description": "my_lib",
        "isExtraImport": true,
        "detail": "my_lib",
        "documentation": {}
    },
    {
        "label": "Object2",
        "importPath": "my_lib",
        "description": "my_lib",
        "isExtraImport": true,
        "detail": "my_lib",
        "documentation": {}
    },
    {
        "label": "lib15",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib1",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib2",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib3",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib4",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib5",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib6",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib7",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib8",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib9",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib10",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib11",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib12",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib13",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib14",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "lib3",
        "importPath": "third_party",
        "description": "third_party",
        "isExtraImport": true,
        "detail": "third_party",
        "documentation": {}
    },
    {
        "label": "google.protobuf.descriptor_pb2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "google.protobuf.descriptor_pb2",
        "description": "google.protobuf.descriptor_pb2",
        "detail": "google.protobuf.descriptor_pb2",
        "documentation": {}
    },
    {
        "label": "Bar",
        "importPath": "source",
        "description": "source",
        "isExtraImport": true,
        "detail": "source",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "scrub",
        "kind": 2,
        "importPath": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "description": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "peekOfCode": "def scrub(s):\n    s = s.replace(endash, \"--\")\n    s = s.replace(emdash, \"--\")\n    return s\ndef process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:\n        for row in tbl(\"tr\"):\n            tds = row(\"td\")\n            # example:",
        "detail": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "process_tables",
        "kind": 2,
        "importPath": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "description": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "peekOfCode": "def process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:\n        for row in tbl(\"tr\"):\n            tds = row(\"td\")\n            # example:\n            # [<td></td>,\n            # <td valign=\"top\"><a href=\"Options.html#index-C_005fPRECEDENCES\"><code>C_PRECEDENCES</code></a></td>,\n            # <td> </td>,\n            # <td valign=\"top\"><a href=\"Options.html#Scripts-and-Functions\">16.2.9 Scripts and Functions</a></td>]",
        "detail": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "save_zdoc_entries",
        "kind": 2,
        "importPath": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "description": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "peekOfCode": "def save_zdoc_entries(prjdir, zdoc_entries):\n    zdoc_entries_sorted = sorted(zdoc_entries, key=lambda x: x.phrase.lower())\n    zman_data_path = prjdir / \"docs\" / \"zmandata.tsv\"\n    print(f\"writing {zman_data_path.name}\")\n    with zman_data_path.open('w') as f:\n        f.write(\"\\t\".join(IndexEntry._fields) + \"\\n\")\n        for e in zdoc_entries_sorted:\n            f.write(\"\\t\".join(e) + \"\\n\")\n    zman_dict = {}\n    for e in zdoc_entries_sorted:",
        "detail": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "description": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "peekOfCode": "def main():\n    prjdir = Path(__file__).absolute().parent.parent\n    zmanindex_classes = {\n        \"concepts\": \"index-cp\",\n        \"variables\": \"index-vr\",\n        \"options\": \"index-pg\",\n        \"functions\": \"index-fn\",\n        \"editor-functions\": \"index-tp\",\n        \"styles-and-tags\": \"index-ky\",\n    }",
        "detail": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "IndexEntry",
        "kind": 5,
        "importPath": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "description": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "peekOfCode": "IndexEntry = namedtuple(\"IndexEntry\", \"phrase category indexed_at url section section_url\")\nendash = '\\u2013'\nemdash = '\\u2014'\ndef scrub(s):\n    s = s.replace(endash, \"--\")\n    s = s.replace(emdash, \"--\")\n    return s\ndef process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:",
        "detail": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "endash",
        "kind": 5,
        "importPath": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "description": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "peekOfCode": "endash = '\\u2013'\nemdash = '\\u2014'\ndef scrub(s):\n    s = s.replace(endash, \"--\")\n    s = s.replace(emdash, \"--\")\n    return s\ndef process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:\n        for row in tbl(\"tr\"):",
        "detail": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "emdash",
        "kind": 5,
        "importPath": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "description": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "peekOfCode": "emdash = '\\u2014'\ndef scrub(s):\n    s = s.replace(endash, \"--\")\n    s = s.replace(emdash, \"--\")\n    return s\ndef process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:\n        for row in tbl(\"tr\"):\n            tds = row(\"td\")",
        "detail": ".cache.antidote.mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "CodeTimer",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class CodeTimer:\n    def __init__(self, name=None):\n        self.name = \" '\" + name + \"'\" if name else \"\"\n    def __enter__(self):\n        self.start = timeit.default_timer()\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.took = (timeit.default_timer() - self.start) * 1000.0\n        print(\"Code block\" + self.name + \" took: \" + str(self.took) + \" ms\")\n### YAML representation\ndef str_presenter(dumper, data):",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DependencyDict",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class DependencyDict(TypedDict):\n    repo: str\n    branch: str\n    version: str\n    precopy: NotRequired[str]\n    postcopy: NotRequired[str]\nclass DependencyYAML(TypedDict):\n    dependencies: dict[str, DependencyDict]\nclass UpdateStatusFalse(TypedDict):\n    has_updates: Literal[False]",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DependencyYAML",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class DependencyYAML(TypedDict):\n    dependencies: dict[str, DependencyDict]\nclass UpdateStatusFalse(TypedDict):\n    has_updates: Literal[False]\nclass UpdateStatusTrue(TypedDict):\n    has_updates: Literal[True]\n    version: str\n    compare_url: str\n    head_ref: str\n    head_url: str",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "UpdateStatusFalse",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class UpdateStatusFalse(TypedDict):\n    has_updates: Literal[False]\nclass UpdateStatusTrue(TypedDict):\n    has_updates: Literal[True]\n    version: str\n    compare_url: str\n    head_ref: str\n    head_url: str\nclass CommandRunner:\n    class Exception(Exception):",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "UpdateStatusTrue",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class UpdateStatusTrue(TypedDict):\n    has_updates: Literal[True]\n    version: str\n    compare_url: str\n    head_ref: str\n    head_url: str\nclass CommandRunner:\n    class Exception(Exception):\n        def __init__(self, message, returncode, stage, stdout, stderr):\n            super().__init__(message)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "CommandRunner",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class CommandRunner:\n    class Exception(Exception):\n        def __init__(self, message, returncode, stage, stdout, stderr):\n            super().__init__(message)\n            self.returncode = returncode\n            self.stage = stage\n            self.stdout = stdout\n            self.stderr = stderr\n    @staticmethod\n    def run_or_fail(command: list[str], stage: str, *args, **kwargs):",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DependencyStore",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class DependencyStore:\n    store: DependencyYAML = {\"dependencies\": {}}\n    @staticmethod\n    def set(data: DependencyYAML):\n        DependencyStore.store = data\n    @staticmethod\n    def update_dependency_version(path: str, version: str) -> DependencyYAML:\n        with CodeTimer(f\"store deepcopy: {path}\"):\n            store_copy = deepcopy(DependencyStore.store)\n        dependency = store_copy[\"dependencies\"].get(path)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "Dependency",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class Dependency:\n    def __init__(self, path: str, values: DependencyDict):\n        self.path = path\n        self.values = values\n        self.name: str = \"\"\n        self.desc: str = \"\"\n        self.kind: str = \"\"\n        match path.split(\"/\"):\n            case [\"plugins\", name]:\n                self.name = name",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "Git",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class Git:\n    default_branch = \"master\"\n    @staticmethod\n    def clone(remote_url: str, branch: str, repo_dir: str, reclone=False):\n        # If repo needs to be fresh\n        if reclone and os.path.exists(repo_dir):\n            shutil.rmtree(repo_dir)\n        # Clone repo in tmp directory and checkout branch\n        if not os.path.exists(repo_dir):\n            print(",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "GitHub",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class GitHub:\n    @staticmethod\n    def check_newer_tag(repo, current_tag) -> UpdateStatusFalse | UpdateStatusTrue:\n        # GET /repos/:owner/:repo/git/refs/tags\n        url = f\"https://api.github.com/repos/{repo}/git/refs/tags\"\n        # Send a GET request to the GitHub API\n        response = requests.get(url)\n        current_version = coerce(current_tag)\n        if current_version is None:\n            raise ValueError(",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "coerce",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "def coerce(version: str) -> Optional[Version]:\n    match = BASEVERSION.search(version)\n    if not match:\n        return None\n    # BASEVERSION looks for `MAJOR.minor.patch` in the string given\n    # it fills with None if any of them is missing (for example `2.1`)\n    ver = {\n        key: 0 if value is None else value for key, value in match.groupdict().items()\n    }\n    # Version takes `major`, `minor`, `patch` arguments",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "str_presenter",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "def str_presenter(dumper, data):\n    \"\"\"\n    Configures yaml for dumping multiline strings\n    Ref: https://stackoverflow.com/a/33300001\n    \"\"\"\n    if len(data.splitlines()) > 1:  # check for multiline string\n        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"|\")\n    return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data)\nyaml.add_representer(str, str_presenter)\nyaml.representer.SafeRepresenter.add_representer(str, str_presenter)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "def main():\n    # Load the YAML file\n    with open(DEPS_YAML_FILE, \"r\") as yaml_file:\n        data: DependencyYAML = yaml.safe_load(yaml_file)\n    if \"dependencies\" not in data:\n        raise Exception(\"dependencies.yml not properly formatted\")\n    # Cache YAML version\n    DependencyStore.set(data)\n    dependencies = data[\"dependencies\"]\n    for path in dependencies:",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "TMP_DIR",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "TMP_DIR = os.path.join(os.environ.get(\"TMP_DIR\", \"/tmp\"), \"ohmyzsh\")\n# Relative path to dependencies.yml file\nDEPS_YAML_FILE = \".github/dependencies.yml\"\n# Dry run flag\nDRY_RUN = os.environ.get(\"DRY_RUN\", \"0\") == \"1\"\n# utils for tag comparison\nBASEVERSION = re.compile(\n    r\"\"\"[vV]?\n    (?P<major>(0|[1-9])\\d*)\n    (\\.",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DEPS_YAML_FILE",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "DEPS_YAML_FILE = \".github/dependencies.yml\"\n# Dry run flag\nDRY_RUN = os.environ.get(\"DRY_RUN\", \"0\") == \"1\"\n# utils for tag comparison\nBASEVERSION = re.compile(\n    r\"\"\"[vV]?\n    (?P<major>(0|[1-9])\\d*)\n    (\\.\n    (?P<minor>(0|[1-9])\\d*)\n    (\\.",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DRY_RUN",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "DRY_RUN = os.environ.get(\"DRY_RUN\", \"0\") == \"1\"\n# utils for tag comparison\nBASEVERSION = re.compile(\n    r\"\"\"[vV]?\n    (?P<major>(0|[1-9])\\d*)\n    (\\.\n    (?P<minor>(0|[1-9])\\d*)\n    (\\.\n    (?P<patch>(0|[1-9])\\d*)\n    )?",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "BASEVERSION",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "BASEVERSION = re.compile(\n    r\"\"\"[vV]?\n    (?P<major>(0|[1-9])\\d*)\n    (\\.\n    (?P<minor>(0|[1-9])\\d*)\n    (\\.\n    (?P<patch>(0|[1-9])\\d*)\n    )?\n    )?\n    \"\"\",",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "peekOfCode": "def parse(line):\n    left = line[0:line.find('=')].strip()\n    right = line[line.find('=')+1:].strip('\\'\"\\n ')\n    try:\n        cmd = next(part for part in right.split() if len([char for char in '=<>' if char in part])==0)\n    except StopIteration:\n        cmd = right\n    return (left, right, cmd)\ndef cheatsheet(lines):\n    exps = [ parse(line) for line in lines ]",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "documentation": {}
    },
    {
        "label": "cheatsheet",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "peekOfCode": "def cheatsheet(lines):\n    exps = [ parse(line) for line in lines ]\n    exps.sort(key=lambda exp:exp[2])\n    cheatsheet = {'_default': []}\n    for key, group in itertools.groupby(exps, lambda exp:exp[2]):\n        group_list = [ item for item in group ]\n        if len(group_list)==1:\n            target_aliases = cheatsheet['_default']\n        else:\n            if key not in cheatsheet:",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "documentation": {}
    },
    {
        "label": "pretty_print_group",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "peekOfCode": "def pretty_print_group(key, aliases, highlight=None, only_groupname=False):\n    if len(aliases) == 0:\n        return\n    group_hl_formatter = lambda g, hl: termcolor.colored(hl, 'yellow').join([termcolor.colored(part, 'red') for part in ('[%s]' % g).split(hl)])\n    alias_hl_formatter = lambda alias, hl: termcolor.colored(hl, 'yellow').join([termcolor.colored(part, 'green') for part in ('\\t%s = %s' % alias[0:2]).split(hl)])\n    group_formatter = lambda g: termcolor.colored('[%s]' % g, 'red')\n    alias_formatter = lambda alias: termcolor.colored('\\t%s = %s' % alias[0:2], 'green')\n    if highlight and len(highlight)>0:\n        print (group_hl_formatter(key, highlight))\n        if not only_groupname:",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "documentation": {}
    },
    {
        "label": "pretty_print",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "peekOfCode": "def pretty_print(cheatsheet, wfilter, group_list=None, groups_only=False):\n    sorted_key = sorted(cheatsheet.keys())\n    for key in sorted_key:\n        if group_list and key not in group_list:\n            continue\n        aliases = cheatsheet.get(key)\n        if not wfilter:\n            pretty_print_group(key, aliases, wfilter, groups_only)\n        else:\n            pretty_print_group(key, [ alias for alias in aliases if alias[0].find(wfilter)>-1 or alias[1].find(wfilter)>-1], wfilter)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "documentation": {}
    },
    {
        "label": "colored",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "def colored(text, color=None, on_color=None, attrs=None):\n    \"\"\"Colorize text.\n    Available text colors:\n        red, green, yellow, blue, magenta, cyan, white.\n    Available text highlights:\n        on_red, on_green, on_yellow, on_blue, on_magenta, on_cyan, on_white.\n    Available attributes:\n        bold, dark, underline, blink, reverse, concealed.\n    Example:\n        colored('Hello, World!', 'red', 'on_grey', ['blue', 'blink'])",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "cprint",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "def cprint(text, color=None, on_color=None, attrs=None, **kwargs):\n    \"\"\"Print colorize text.\n    It accepts arguments of print function.\n    \"\"\"\n    print((colored(text, color, on_color, attrs)), **kwargs)\nif __name__ == '__main__':\n    print('Current terminal type: %s' % os.getenv('TERM'))\n    print('Test basic colors:')\n    cprint('Grey color', 'grey')\n    cprint('Red color', 'red')",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "__ALL__",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "__ALL__ = [ 'colored', 'cprint' ]\nVERSION = (1, 1, 0)\nATTRIBUTES = dict(\n        list(zip([\n            'bold',\n            'dark',\n            '',\n            'underline',\n            'blink',\n            '',",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "VERSION",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "VERSION = (1, 1, 0)\nATTRIBUTES = dict(\n        list(zip([\n            'bold',\n            'dark',\n            '',\n            'underline',\n            'blink',\n            '',\n            'reverse',",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "ATTRIBUTES",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "ATTRIBUTES = dict(\n        list(zip([\n            'bold',\n            'dark',\n            '',\n            'underline',\n            'blink',\n            '',\n            'reverse',\n            'concealed'",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "HIGHLIGHTS",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "HIGHLIGHTS = dict(\n        list(zip([\n            'on_grey',\n            'on_red',\n            'on_green',\n            'on_yellow',\n            'on_blue',\n            'on_magenta',\n            'on_cyan',\n            'on_white'",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "COLORS",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "COLORS = dict(\n        list(zip([\n            'grey',\n            'red',\n            'green',\n            'yellow',\n            'blue',\n            'magenta',\n            'cyan',\n            'white',",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "RESET",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "RESET = '\\033[0m'\ndef colored(text, color=None, on_color=None, attrs=None):\n    \"\"\"Colorize text.\n    Available text colors:\n        red, green, yellow, blue, magenta, cyan, white.\n    Available text highlights:\n        on_red, on_green, on_yellow, on_blue, on_magenta, on_cyan, on_white.\n    Available attributes:\n        bold, dark, underline, blink, reverse, concealed.\n    Example:",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "code_to_omz",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "def code_to_omz(_code_points):\n    \"\"\" Returns a ZSH-compatible Unicode string from the code point(s) \"\"\"\n    return r'\\U' + r'\\U'.join(_code_points.split(' '))\ndef name_to_omz(_name, _group, _subgroup, _status):\n    \"\"\" Returns a reasonable snake_case name for the emoji. \"\"\"\n    def snake_case(_string):\n        \"\"\" Does the regex work of snake_case \"\"\"\n        remove_dots = re.sub(r'\\.\\(\\)', r'', _string)\n        replace_ands = re.sub(r'\\&', r'and', remove_dots)\n        remove_whitespace = re.sub(r'[^\\#\\*\\w]', r'_', replace_ands)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "name_to_omz",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "def name_to_omz(_name, _group, _subgroup, _status):\n    \"\"\" Returns a reasonable snake_case name for the emoji. \"\"\"\n    def snake_case(_string):\n        \"\"\" Does the regex work of snake_case \"\"\"\n        remove_dots = re.sub(r'\\.\\(\\)', r'', _string)\n        replace_ands = re.sub(r'\\&', r'and', remove_dots)\n        remove_whitespace = re.sub(r'[^\\#\\*\\w]', r'_', replace_ands)\n        return re.sub(r'__', r'_', remove_whitespace)\n    shortname = \"\"\n    split_at_colon = lambda s: s.split(\": \")",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "increment_name",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "def increment_name(_shortname):\n    \"\"\" Increment the short name by 1. If you get, say,\n    'woman_detective_unqualified', it returns\n    'woman_detective_unqualified_1', and then\n    'woman_detective_unqualified_2', etc. \"\"\"\n    last_char = _shortname[-1]\n    if last_char.isdigit():\n        num = int(last_char)\n        return _shortname[:-1] + str(num + 1)\n    return _shortname + \"_1\"",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "spec",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "spec = open(\"emoji-data.txt\", \"r\")\n# Regexes\n# regex_emoji will return, respectively:\n# the code points, its type (status), the actual emoji, and its official name\nregex_emoji = r\"^([\\w ].*?\\S)\\s*;\\s*([\\w-]+)\\s*#\\s*(.*?)\\s(\\S.*).*$\"\n# regex_group returns the group of subgroup that a line opens\nregex_group = r\"^#\\s*(group|subgroup):\\s*(.*)$\"\nheaders = \"\"\"\n# emoji-char-definitions.zsh - Emoji definitions for oh-my-zsh emoji plugin\n#",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "regex_emoji",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "regex_emoji = r\"^([\\w ].*?\\S)\\s*;\\s*([\\w-]+)\\s*#\\s*(.*?)\\s(\\S.*).*$\"\n# regex_group returns the group of subgroup that a line opens\nregex_group = r\"^#\\s*(group|subgroup):\\s*(.*)$\"\nheaders = \"\"\"\n# emoji-char-definitions.zsh - Emoji definitions for oh-my-zsh emoji plugin\n#\n# This file is auto-generated by update_emoji.py. Do not edit it manually.\n#\n# This contains the definition for:\n#   $emoji         - which maps character names to Unicode characters",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "regex_group",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "regex_group = r\"^#\\s*(group|subgroup):\\s*(.*)$\"\nheaders = \"\"\"\n# emoji-char-definitions.zsh - Emoji definitions for oh-my-zsh emoji plugin\n#\n# This file is auto-generated by update_emoji.py. Do not edit it manually.\n#\n# This contains the definition for:\n#   $emoji         - which maps character names to Unicode characters\n#   $emoji_flags   - maps country names to Unicode flag characters using region\n#                    indicators",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "headers",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "headers = \"\"\"\n# emoji-char-definitions.zsh - Emoji definitions for oh-my-zsh emoji plugin\n#\n# This file is auto-generated by update_emoji.py. Do not edit it manually.\n#\n# This contains the definition for:\n#   $emoji         - which maps character names to Unicode characters\n#   $emoji_flags   - maps country names to Unicode flag characters using region\n#                    indicators\n#   $emoji_mod     - maps modifier components to Unicode characters",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "emoji_database",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "emoji_database = []\nfor line in spec:\n    # First, test if this line opens a group or subgroup\n    group_match = re.findall(regex_group, line)\n    if group_match != []:\n        gr_or_sub, name = group_match[0]\n        if gr_or_sub == \"group\":\n            group = name\n        elif gr_or_sub == \"subgroup\":\n            subgroup = name",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "gemoji_db",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "gemoji_db = open(\"gemoji_db.json\")\nj = json.load(gemoji_db)\naliases_map = {entry['emoji']: entry['aliases'] for entry in j}\nall_omz_names = [emoji_data[3] for emoji_data in emoji_database]\n# Let's begin writing to this file\noutput = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "j",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "j = json.load(gemoji_db)\naliases_map = {entry['emoji']: entry['aliases'] for entry in j}\nall_omz_names = [emoji_data[3] for emoji_data in emoji_database]\n# Let's begin writing to this file\noutput = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "aliases_map",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "aliases_map = {entry['emoji']: entry['aliases'] for entry in j}\nall_omz_names = [emoji_data[3] for emoji_data in emoji_database]\n# Let's begin writing to this file\noutput = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down\nfor _omz_codes, _status, _emoji, _omz_name, _group, _subgroup in emoji_database:",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "all_omz_names",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "all_omz_names = [emoji_data[3] for emoji_data in emoji_database]\n# Let's begin writing to this file\noutput = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down\nfor _omz_codes, _status, _emoji, _omz_name, _group, _subgroup in emoji_database:\n    # One emoji can be mapped to multiple names (aliases or country codes)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "output = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down\nfor _omz_codes, _status, _emoji, _omz_name, _group, _subgroup in emoji_database:\n    # One emoji can be mapped to multiple names (aliases or country codes)\n    names_for_this_emoji = [_omz_name]\n    # Variable that indicates in which map the emoji will be located",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "emoji_groups",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "emoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down\nfor _omz_codes, _status, _emoji, _omz_name, _group, _subgroup in emoji_database:\n    # One emoji can be mapped to multiple names (aliases or country codes)\n    names_for_this_emoji = [_omz_name]\n    # Variable that indicates in which map the emoji will be located\n    emoji_map = \"emoji\"\n    if _status == \"component\":",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "get_tagname_or_hash",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "def get_tagname_or_hash():\n    \"\"\"return tagname if exists else hash\"\"\"\n    # get hash\n    hash_cmd = ['git', 'rev-parse', '--short', 'HEAD']\n    hash_ = check_output(hash_cmd).decode('utf-8').strip()\n    # get tagname\n    tags_cmd = ['git', 'for-each-ref', '--points-at=HEAD', '--count=2', '--sort=-version:refname', '--format=%(refname:short)', 'refs/tags']\n    tags = check_output(tags_cmd).decode('utf-8').split()\n    if tags:\n        return tags[0] + ('+' if len(tags) > 1 else '')",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "get_stash",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "def get_stash():\n    cmd = Popen(['git', 'rev-parse', '--git-dir'], stdout=PIPE, stderr=PIPE)\n    so, se = cmd.communicate()\n    stash_file = '%s%s' % (so.decode('utf-8').rstrip(), '/logs/refs/stash')\n    try:\n        with open(stash_file) as f:\n            return sum(1 for _ in f)\n    except IOError:\n        return 0\n# `git status --porcelain --branch` can collect all information",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "po",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "po = Popen(['git', 'status', '--porcelain', '--branch'], env=dict(os.environ, LANG=\"C\"), stdout=PIPE, stderr=PIPE)\nstdout, sterr = po.communicate()\nif po.returncode != 0:\n    sys.exit(0)  # Not a git repository\n# collect git status information\nuntracked, staged, changed, deleted, conflicts = [], [], [], [], []\nahead, behind = 0, 0\nstatus = [(line[0], line[1], line[2:]) for line in stdout.decode('utf-8').splitlines()]\nfor st in status:\n    if st[0] == '#' and st[1] == '#':",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "status",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "status = [(line[0], line[1], line[2:]) for line in stdout.decode('utf-8').splitlines()]\nfor st in status:\n    if st[0] == '#' and st[1] == '#':\n        if re.search('Initial commit on', st[2]) or re.search('No commits yet on', st[2]):\n            branch = st[2].split(' ')[-1]\n        elif re.search('no branch', st[2]):  # detached status\n            branch = get_tagname_or_hash()\n        elif len(st[2].strip().split('...')) == 1:\n            branch = st[2].strip()\n        else:",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "stashed",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "stashed = get_stash()\nif not changed and not deleted and not staged and not conflicts and not untracked:\n    clean = 1\nelse:\n    clean = 0\nout = ' '.join([\n    branch,\n    str(ahead),\n    str(behind),\n    str(len(staged)),",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "out",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "out = ' '.join([\n    branch,\n    str(ahead),\n    str(behind),\n    str(len(staged)),\n    str(len(conflicts)),\n    str(len(changed)),\n    str(len(untracked)),\n    str(stashed),\n    str(clean),",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "CommandSet",
        "kind": 6,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "class CommandSet:\n    proxies = make_proxies(*get_http_proxy())\n    aliases = {\n        _: \"env __SSH_PROGRAM_NAME__=%s %s\" % (_, ssh_agent)\n        for _ in (\"ssh\", \"sftp\", \"scp\", \"slogin\", \"ssh-copy-id\")\n    }\n    def enable(self):\n        cmdline(\"export\", *merge(self.proxies))\n        cmdline(\"alias\", *merge(self.aliases))\n    def disable(self):",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "get_http_proxy",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)\n    if default_proxy and no_proxy:\n        return default_proxy, no_proxy\n    if os.path.isfile(proxy_config):\n        proxy_configdata = [line.strip() for line in check_output(proxy_config).decode(\"utf-8\").splitlines()]\n        if len(proxy_configdata) >= 1:\n            if not default_proxy:\n                default_proxy = proxy_configdata[0]",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "make_proxies",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def make_proxies(url: str, no_proxy: str):\n    proxies = {\"%s_PROXY\" % _: url for _ in (\"HTTP\", \"HTTPS\", \"FTP\", \"RSYNC\", \"ALL\")}\n    proxies.update({name.lower(): value for (name, value) in proxies.items()})\n    proxies[\"GIT_SSH\"] = ssh_agent\n    if no_proxy:\n        proxies.update({\"NO_PROXY\": no_proxy, \"no_proxy\": no_proxy})\n    return proxies\ndef merge(mapping: dict):\n    return (\"%s=%s\" % _ for _ in mapping.items())\nclass CommandSet:",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "merge",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def merge(mapping: dict):\n    return (\"%s=%s\" % _ for _ in mapping.items())\nclass CommandSet:\n    proxies = make_proxies(*get_http_proxy())\n    aliases = {\n        _: \"env __SSH_PROGRAM_NAME__=%s %s\" % (_, ssh_agent)\n        for _ in (\"ssh\", \"sftp\", \"scp\", \"slogin\", \"ssh-copy-id\")\n    }\n    def enable(self):\n        cmdline(\"export\", *merge(self.proxies))",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "cmdline",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def cmdline(*items):\n    print(list2cmdline(items))\ndef main():\n    command = CommandSet()\n    if len(sys.argv) == 1:\n        command.usage()\n        sys.exit(1)\n    getattr(command, sys.argv[1], command.usage)()\nif __name__ == \"__main__\":\n    main()",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def main():\n    command = CommandSet()\n    if len(sys.argv) == 1:\n        command.usage()\n        sys.exit(1)\n    getattr(command, sys.argv[1], command.usage)()\nif __name__ == \"__main__\":\n    main()",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "cwd",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "cwd = os.path.dirname(__file__)\nssh_agent = os.path.join(cwd, \"ssh-agent.py\")\nproxy_env = \"SHELLPROXY_URL\"\nno_proxy_env = \"SHELLPROXY_NO_PROXY\"\nproxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "ssh_agent",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "ssh_agent = os.path.join(cwd, \"ssh-agent.py\")\nproxy_env = \"SHELLPROXY_URL\"\nno_proxy_env = \"SHELLPROXY_NO_PROXY\"\nproxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "proxy_env",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "proxy_env = \"SHELLPROXY_URL\"\nno_proxy_env = \"SHELLPROXY_NO_PROXY\"\nproxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)\n    if default_proxy and no_proxy:",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "no_proxy_env",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "no_proxy_env = \"SHELLPROXY_NO_PROXY\"\nproxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)\n    if default_proxy and no_proxy:\n        return default_proxy, no_proxy",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "proxy_config",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "proxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)\n    if default_proxy and no_proxy:\n        return default_proxy, no_proxy\n    if os.path.isfile(proxy_config):",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "ssh_proxy",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "peekOfCode": "ssh_proxy = os.path.join(os.path.dirname(__file__), \"ssh-proxy.py\")\nargv = [\n    os.environ.get(\"__SSH_PROGRAM_NAME__\", \"ssh\"),\n    \"-o\",\n    \"ProxyCommand={} %h %p\".format(ssh_proxy),\n    \"-o\",\n    \"Compression=yes\",\n]\nsubprocess.call(argv + sys.argv[1:], env=os.environ)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "documentation": {}
    },
    {
        "label": "argv",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "peekOfCode": "argv = [\n    os.environ.get(\"__SSH_PROGRAM_NAME__\", \"ssh\"),\n    \"-o\",\n    \"ProxyCommand={} %h %p\".format(ssh_proxy),\n    \"-o\",\n    \"Compression=yes\",\n]\nsubprocess.call(argv + sys.argv[1:], env=os.environ)",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "documentation": {}
    },
    {
        "label": "make_argv",
        "kind": 2,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "peekOfCode": "def make_argv():\n    yield \"nc\"\n    if sys.platform in {'linux', 'cygwin'}:\n        # caveats: the built-in netcat of most linux distributions and cygwin support proxy type\n        # caveats: macOS built-in netcat command not supported proxy-type\n        yield \"-X\" # --proxy-type\n        # Supported protocols are 4 (SOCKS v4), 5 (SOCKS v5) and connect (HTTP proxy).\n        # Default SOCKS v5 is used.\n        yield proxy_protocols[parsed.scheme]\n    yield \"-x\" # --proxy",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "documentation": {}
    },
    {
        "label": "proxy",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "peekOfCode": "proxy = next(os.environ[_] for _ in (\"HTTP_PROXY\", \"HTTPS_PROXY\") if _ in os.environ)\nparsed = urlparse(proxy)\nproxy_protocols = {\n    \"http\": \"connect\",\n    \"https\": \"connect\",\n    \"socks\": \"5\",\n    \"socks5\": \"5\",\n    \"socks4\": \"4\",\n    \"socks4a\": \"4\",\n}",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "documentation": {}
    },
    {
        "label": "parsed",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "peekOfCode": "parsed = urlparse(proxy)\nproxy_protocols = {\n    \"http\": \"connect\",\n    \"https\": \"connect\",\n    \"socks\": \"5\",\n    \"socks5\": \"5\",\n    \"socks4\": \"4\",\n    \"socks4a\": \"4\",\n}\nif parsed.scheme not in proxy_protocols:",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "documentation": {}
    },
    {
        "label": "proxy_protocols",
        "kind": 5,
        "importPath": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "description": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "peekOfCode": "proxy_protocols = {\n    \"http\": \"connect\",\n    \"https\": \"connect\",\n    \"socks\": \"5\",\n    \"socks5\": \"5\",\n    \"socks4\": \"4\",\n    \"socks4a\": \"4\",\n}\nif parsed.scheme not in proxy_protocols:\n    raise TypeError('unsupported proxy protocol: \"{}\"'.format(parsed.scheme))",
        "detail": ".cache.antidote.ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "documentation": {}
    },
    {
        "label": "Example3",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "peekOfCode": "class Example3(   object ):\n    def __init__    ( self, bar ):\n     #Comments should have a space after the hash.\n     if bar : bar+=1;  bar=bar* bar   ; return bar\n     else:\n                    some_string = \"\"\"\n                       Indentation in multiline strings should not be touched.\nOnly actual code should be reindented.\n\"\"\"\n                    return (sys.path, some_string)",
        "detail": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "example1",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "peekOfCode": "def example1():\n    ####This is a long comment. This should be wrapped to fit within 72 characters.\n    some_tuple=(   1,2, 3,'a'  );\n    some_variable={'long':'Long code lines should be wrapped within 79 characters.',\n    'other':[math.pi, 100,200,300,9876543210,'This is a long string that goes on'],\n    'more':{'inner':'This whole logical line should be wrapped.',some_tuple:[1,\n    20,300,40000,500000000,60000000000000000]}}\n    return (some_tuple, some_variable)\ndef example2(): return {'has_key() is deprecated':True}.has_key({'f':2}.has_key(''));\nclass Example3(   object ):",
        "detail": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "example2",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "peekOfCode": "def example2(): return {'has_key() is deprecated':True}.has_key({'f':2}.has_key(''));\nclass Example3(   object ):\n    def __init__    ( self, bar ):\n     #Comments should have a space after the hash.\n     if bar : bar+=1;  bar=bar* bar   ; return bar\n     else:\n                    some_string = \"\"\"\n                       Indentation in multiline strings should not be touched.\nOnly actual code should be reindented.\n\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.autopep8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "pick",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "peekOfCode": "pick = dill.dumps({\"a\": \"b\", \"c\": \"d\"})\nprint(dill.loads(pick))\nfile_obj = StringIO.StringIO()\ndill.dump([1, 2, \"3\"], file_obj)",
        "detail": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "file_obj",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "peekOfCode": "file_obj = StringIO.StringIO()\ndill.dump([1, 2, \"3\"], file_obj)",
        "detail": ".trunk.plugins.trunk.linters.bandit.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n    def __inti__(self):\n        prit(\"this is not a valid method\")\n        callbak = lamda x: x * 2\n    varName1 = \"helol ym anme is var\"",
        "detail": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n    def __inti__(self):\n        prit(\"this is not a valid method\")\n        callbak = lamda x: x * 2\n    varName1 = \"helol ym anme is var\"\ncachedir = \"/tmp\"",
        "detail": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\nimport sys\nimport pands\nclass NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1",
        "detail": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "cachedir",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "peekOfCode": "cachedir = \"/tmp\"",
        "detail": ".trunk.plugins.trunk.linters.codespell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "description": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "description": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "peekOfCode": "def main(argv):\n    results = []\n    for line in sys.stdin.readlines():\n        filename, line_number, message = line.split(\":\")\n        results.append(\n            to_result_sarif(\n                filename, int(line_number), 0, \"misspelled\", message.strip()\n            )\n        )\n    sarif = {",
        "detail": ".trunk.plugins.trunk.linters.codespell.codespell_to_sarif",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n    def __inti__(self):\n        prit(\"this is not a valid method\")\n        callbak = lamda x: x * 2\n    varName1 = \"helol ym anme is var\"",
        "detail": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n    def __inti__(self):\n        prit(\"this is not a valid method\")\n        callbak = lamda x: x * 2\n    varName1 = \"helol ym anme is var\"\ncachedir = \"/tmp\"",
        "detail": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\nimport sys\nimport pands\nclass NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1",
        "detail": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "cachedir",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "description": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "peekOfCode": "cachedir = \"/tmp\"",
        "detail": ".trunk.plugins.trunk.linters.cspell.test_data.basic_py.in",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'",
        "detail": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'",
        "detail": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\nimport sys\n# trunk-ignore(flake8/F401): this will trigger a warning to verify that the config is applied\nclass NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1",
        "detail": ".trunk.plugins.trunk.linters.flake8.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "aws_access_key_id",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "peekOfCode": "aws_access_key_id = \"AKIAIO5FODNN7EXAMPLE\"\naws_access_key_id = \"AKIAIO5FODNN7EXAMPLE\"",
        "detail": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "documentation": {}
    },
    {
        "label": "aws_access_key_id",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "peekOfCode": "aws_access_key_id = \"AKIAIO5FODNN7EXAMPLE\"",
        "detail": ".trunk.plugins.trunk.linters.gitleaks.test_data.basic",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "description": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "peekOfCode": "def to_result_sarif(path: str, lineno: int, colno: int, rule_id: str, message: str):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "description": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "peekOfCode": "def main(argv):\n    output_json = json.load(sys.stdin)\n    errors = output_json.get(\"errors\", [])\n    results = []\n    for error in errors:\n        rule = error.get(\"rule\", \"\")\n        message = error.get(\"message\", \"\")\n        location = error.get(\"location\")\n        if location:\n            path = location.get(\"file\", \"\")",
        "detail": ".trunk.plugins.trunk.linters.graphql-schema-linter.parse",
        "documentation": {}
    },
    {
        "label": "try_find_string_in_file",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "description": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "peekOfCode": "def try_find_string_in_file(filename, search_string):\n    with open(filename, \"r\") as f:\n        for i, line in enumerate(f):\n            index = line.find(search_string)\n            if index != -1:\n                return i + 1, index + 1\n    return 0, 0\ndef to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):",
        "detail": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "description": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "description": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "peekOfCode": "def main(argv):\n    parser = argparse.ArgumentParser(description=\"Parse output of markdown-link-check\")\n    parser.add_argument(\"--target\", dest=\"target\")\n    args = parser.parse_args()\n    results = []\n    # Line numbers are not reported out of the tool right now - so we regex parse the output to extract issue codes\n    for line in sys.stdin:\n        parse_reg = \"\\s*(\\[.*\\])\\s(.*)→.*Status:\\s*(\\d*)(.*)\"\n        filename = args.target\n        parse_result = re.fullmatch(parse_reg, line, flags=re.DOTALL)",
        "detail": ".trunk.plugins.trunk.linters.markdown-link-check.parse",
        "documentation": {}
    },
    {
        "label": "greeting",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "peekOfCode": "def greeting(name: str) -> str:\n    return \"Hello \" + name\ndef printer() -> None:\n    print(\"Hello\")\ngreeting(3)\ngreeting(b\"Alice\")\na = printer()\nc: str = 4\nfrom source import Bar\ndef bad_foo(bar: Bar) -> str:",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "documentation": {}
    },
    {
        "label": "printer",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "peekOfCode": "def printer() -> None:\n    print(\"Hello\")\ngreeting(3)\ngreeting(b\"Alice\")\na = printer()\nc: str = 4\nfrom source import Bar\ndef bad_foo(bar: Bar) -> str:\n  return bar.a + bar.b",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "documentation": {}
    },
    {
        "label": "bad_foo",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "peekOfCode": "def bad_foo(bar: Bar) -> str:\n  return bar.a + bar.b",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "peekOfCode": "a = printer()\nc: str = 4\nfrom source import Bar\ndef bad_foo(bar: Bar) -> str:\n  return bar.a + bar.b",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.basic",
        "documentation": {}
    },
    {
        "label": "Bar",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "peekOfCode": "class Bar:\n  a: int\n  b: int\ndef bad_function() -> int:\n  print(\"returns nothing\")",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "documentation": {}
    },
    {
        "label": "bad_function",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "description": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "peekOfCode": "def bad_function() -> int:\n  print(\"returns nothing\")",
        "detail": ".trunk.plugins.trunk.linters.mypy.test_data.source",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.nancy.parse",
        "description": ".trunk.plugins.trunk.linters.nancy.parse",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.nancy.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.nancy.parse",
        "description": ".trunk.plugins.trunk.linters.nancy.parse",
        "peekOfCode": "def main(argv):\n    results = []\n    nancy_output = json.load(sys.stdin)\n    for vuln_entry in nancy_output.get(\"vulnerable\", []):\n        for vuln in vuln_entry.get(\"Vulnerabilities\", []):\n            results.append(\n                to_result_sarif(\n                    \".\",\n                    0,\n                    0,",
        "detail": ".trunk.plugins.trunk.linters.nancy.parse",
        "documentation": {}
    },
    {
        "label": "get_sarif_severity",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"database_specific\" not in vuln:\n        return DEFAULT_SARIF_SEVERITY\n    vuln_metadata = vuln[\"database_specific\"]\n    if \"severity\" not in vuln_metadata:\n        return DEFAULT_SARIF_SEVERITY\n    severity = vuln_metadata[\"severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def to_result_sarif(\n    path: str, lineno: int, vuln_id: str, description: str, severity: str\n):\n    return {\n        \"level\": severity,\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "join_common_sets",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def join_common_sets(lst):\n    init_len = 0\n    final_len = 1\n    while init_len != final_len:\n        init_len = len(lst)\n        ret = []\n        for s in lst:\n            unique = True\n            for stored_set in ret:\n                if len(stored_set.intersection(s)) > 0:",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "get_preferred_alias",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def get_preferred_alias(aliases):\n    for rx in PREFERRED_ORDER:\n        found_aliases = sorted(alias for alias in aliases if re.match(rx, alias))\n        if len(found_aliases) > 0:\n            return found_aliases[0]\n    return sorted(aliases)[0]\ndef main(argv):\n    try:\n        # On Windows, Unicode characters in the osv-scanner output cause json parsing errors. Filter them out since we don't care about their fields.\n        if sys.platform == \"win32\":",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "def main(argv):\n    try:\n        # On Windows, Unicode characters in the osv-scanner output cause json parsing errors. Filter them out since we don't care about their fields.\n        if sys.platform == \"win32\":\n            filtered_stdin = \"\".join(i for i in sys.stdin.read() if ord(i) < 256)\n            osv_json = json.loads(filtered_stdin)\n        else:\n            osv_json = json.load(sys.stdin)\n    except json.decoder.JSONDecodeError as err:\n        if str(err) == \"Expecting value: line 1 column 1 (char 0)\":",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "SARIF_SEVERITY_BY_OSV_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "SARIF_SEVERITY_BY_OSV_SEVERITY = {\n    \"CRITICAL\": \"error\",\n    \"HIGH\": \"error\",\n    \"MODERATE\": \"warning\",\n    \"MEDIUM\": \"warning\",\n    \"LOW\": \"note\",\n}\nDEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SARIF_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "DEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"database_specific\" not in vuln:\n        return DEFAULT_SARIF_SEVERITY\n    vuln_metadata = vuln[\"database_specific\"]\n    if \"severity\" not in vuln_metadata:\n        return DEFAULT_SARIF_SEVERITY\n    severity = vuln_metadata[\"severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "PREFERRED_ORDER",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "description": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "peekOfCode": "PREFERRED_ORDER = [\"GHSA-.*\", \"CVE-.*\", \"PYSEC-.*\"]\ndef get_preferred_alias(aliases):\n    for rx in PREFERRED_ORDER:\n        found_aliases = sorted(alias for alias in aliases if re.match(rx, alias))\n        if len(found_aliases) > 0:\n            return found_aliases[0]\n    return sorted(aliases)[0]\ndef main(argv):\n    try:\n        # On Windows, Unicode characters in the osv-scanner output cause json parsing errors. Filter them out since we don't care about their fields.",
        "detail": ".trunk.plugins.trunk.linters.osv-scanner.osv_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.phpstan.phpstan_parser",
        "description": ".trunk.plugins.trunk.linters.phpstan.phpstan_parser",
        "peekOfCode": "def main():\n    phpstan_json = json.loads(sys.stdin.read())\n    results = []\n    for file_name in phpstan_json[\"files\"]:\n        file_result = phpstan_json[\"files\"][file_name]\n        for result in file_result[\"messages\"]:\n            result = {\n                # We do not have a ruleId\n                \"message\": {\n                    \"text\": result[\"message\"],",
        "detail": ".trunk.plugins.trunk.linters.phpstan.phpstan_parser",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "description": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "peekOfCode": "def to_result_sarif(path: str, description: str, line: int = 0, column: int = 0):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "description": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "peekOfCode": "def main(argv):\n    if len(argv) < 2:\n        print(\"Usage: trivy_to_sarif.py <exit_code>)\")\n        sys.exit(1)\n    if argv[1] == \"0\":\n        results = []\n        sarif = {\n            \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n            \"version\": \"2.1.0\",\n            \"runs\": [{\"results\": results}],",
        "detail": ".trunk.plugins.trunk.linters.prettier.prettier_to_sarif",
        "documentation": {}
    },
    {
        "label": "shift",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "shift = 3\nchoice = raw_input(\"would you like to encode or decode?\")\nword = raw_input(\"Please enter text\")\nletters = string.ascii_letters + string.punctuation + string.digits\nencoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "choice",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "choice = raw_input(\"would you like to encode or decode?\")\nword = raw_input(\"Please enter text\")\nletters = string.ascii_letters + string.punctuation + string.digits\nencoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "word",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "word = raw_input(\"Please enter text\")\nletters = string.ascii_letters + string.punctuation + string.digits\nencoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "letters",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "letters = string.ascii_letters + string.punctuation + string.digits\nencoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "encoded",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "peekOfCode": "encoded = \"\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "foo",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.pylint.test_data.severity",
        "description": ".trunk.plugins.trunk.linters.pylint.test_data.severity",
        "peekOfCode": "def foo():\n    return \"bar\"",
        "detail": ".trunk.plugins.trunk.linters.pylint.test_data.severity",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "class A:\n    def method1(self) -> None:\n        self.x = 1\n    def method2(self) -> None:\n        self.x = \"\" # Mypy treats this as an error because `x` is implicitly declared as `int`\na = A()\nreveal_type(a.x)\na.x = \"\" # Pyright allows this because the type of `x` is `int | str`\na.x = 3.0 # Pyright treats this as an error because the type of `x` is `int | str`\nclass A:",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "class A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)\nprint(A.y)\nprint(A.z) # pyright shows error, mypy shows no error\nclass Color(Enum):\n    RED = 1",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "Color",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "class Color(Enum):\n    RED = 1\n    BLUE = 2\ndef is_red(color: Color) -> bool:\n    if color == Color.RED:\n        return True\n    elif color == Color.BLUE:\n        return False\n    # mypy reports error: Missing return statement\ndef func(val: int | None):",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "wrong_type",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "def wrong_type(x: int) -> str:\n    return x  # error: Incompatible return value type (got \"int\", expected \"str\")\nclass A:\n    def method1(self) -> None:\n        self.x = 1\n    def method2(self) -> None:\n        self.x = \"\" # Mypy treats this as an error because `x` is implicitly declared as `int`\na = A()\nreveal_type(a.x)\na.x = \"\" # Pyright allows this because the type of `x` is `int | str`",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "is_red",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "def is_red(color: Color) -> bool:\n    if color == Color.RED:\n        return True\n    elif color == Color.BLUE:\n        return False\n    # mypy reports error: Missing return statement\ndef func(val: int | None):\n    if val is not None:\n        def inner_1() -> None:\n            reveal_type(val)",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "func",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "def func(val: int | None):\n    if val is not None:\n        def inner_1() -> None:\n            reveal_type(val)\n            print(val + 1)  # mypy produces a false positive error here\n        inner_2 = lambda: reveal_type(val) + 1\n        inner_1()\n        inner_2()",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "a = A()\nreveal_type(a.x)\na.x = \"\" # Pyright allows this because the type of `x` is `int | str`\na.x = 3.0 # Pyright treats this as an error because the type of `x` is `int | str`\nclass A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "a.x",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "a.x = \"\" # Pyright allows this because the type of `x` is `int | str`\na.x = 3.0 # Pyright treats this as an error because the type of `x` is `int | str`\nclass A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)\nprint(A.y)\nprint(A.z) # pyright shows error, mypy shows no error",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "a.x",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "peekOfCode": "a.x = 3.0 # Pyright treats this as an error because the type of `x` is `int | str`\nclass A:\n    x: int = 0 # Regular class variable\n    y: ClassVar[int] = 0 # Pure class variable\n    def __init__(self):\n        self.z = 0 # Pure instance variable\nprint(A.x)\nprint(A.y)\nprint(A.z) # pyright shows error, mypy shows no error\nclass Color(Enum):",
        "detail": ".trunk.plugins.trunk.linters.pyright.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "description": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "peekOfCode": "results = []\nfor result in json.load(sys.stdin)[\"generalDiagnostics\"]:\n    parse = {\n        \"level\": result[\"severity\"] if result[\"severity\"] != \"information\" else \"note\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": result[\"file\"],\n                    },",
        "detail": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "documentation": {}
    },
    {
        "label": "sarif",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "description": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "peekOfCode": "sarif = {\n    \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n    \"version\": \"2.1.0\",\n    \"runs\": [{\"results\": results}],\n}\nprint(json.dumps(sarif, indent=2))",
        "detail": ".trunk.plugins.trunk.linters.pyright.pyright_to_sarif",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "description": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "description": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "peekOfCode": "def main(argv):\n    results = []\n    content_json = sys.stdin.read()\n    content = json.loads(content_json)\n    for file_content in content:\n        messages = file_content.get(\"messages\", [])\n        if messages:\n            for msg in messages:\n                results.append(\n                    to_result_sarif(",
        "detail": ".trunk.plugins.trunk.linters.remark-lint.parse",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.renovate.parse",
        "description": ".trunk.plugins.trunk.linters.renovate.parse",
        "peekOfCode": "def to_result_sarif(\n    path: str, line_number: int, column_number: int, rule_id: str, message: str\n):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.renovate.parse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.renovate.parse",
        "description": ".trunk.plugins.trunk.linters.renovate.parse",
        "peekOfCode": "def main(argv):\n    results = []\n    content = sys.stdin.read()\n    parse_reg = \"(.*WARN:.*could not be parsed)(.*)\"\n    error_section = content.find('\"errors\": [')\n    parse_result = re.fullmatch(parse_reg, content, flags=re.DOTALL)\n    if parse_result:\n        warn_section = parse_result.group(2)\n        json_content = \"{\" + warn_section + \"}\"\n        error_output = json.loads(json_content)",
        "detail": ".trunk.plugins.trunk.linters.renovate.parse",
        "documentation": {}
    },
    {
        "label": "map_severity",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "description": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "peekOfCode": "def map_severity(severity):\n    if severity in [\"convention\", \"refactor\", \"info\"]:\n        return \"note\"\n    if severity in [\"warning\"]:\n        return \"warning\"\n    if severity in [\"error\", \"fatal\"]:\n        return \"error\"\n    return \"none\"\nresults = []\nfor file in json.load(sys.stdin)[\"files\"]:",
        "detail": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "description": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "peekOfCode": "results = []\nfor file in json.load(sys.stdin)[\"files\"]:\n    for offense in file[\"offenses\"]:\n        parse = {\n            \"level\": map_severity(offense[\"severity\"]),\n            \"locations\": [\n                {\n                    \"physicalLocation\": {\n                        \"artifactLocation\": {\n                            \"uri\": file[\"path\"],",
        "detail": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "documentation": {}
    },
    {
        "label": "sarif",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "description": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "peekOfCode": "sarif = {\n    \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n    \"version\": \"2.1.0\",\n    \"runs\": [{\"results\": results}],\n}\nprint(json.dumps(sarif, indent=2))",
        "detail": ".trunk.plugins.trunk.linters.rubocop.rubocop_to_sarif",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'",
        "detail": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'",
        "detail": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\nimport sys\n# trunk-ignore(ruff/F401)\nimport json\nclass NoDocstring(object):\n    def __init__(self, arg1):",
        "detail": ".trunk.plugins.trunk.linters.ruff.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "get_region",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "peekOfCode": "def get_region(entry, column_offset=0):\n    location = entry[\"location\"]\n    region = {\n        \"startColumn\": location[\"column\"] + column_offset,\n        \"startLine\": location[\"row\"],\n    }\n    if \"end_location\" in entry:\n        end_location = entry[\"end_location\"]\n        region[\"endColumn\"] = end_location[\"column\"] + column_offset\n        region[\"endLine\"] = end_location[\"row\"]",
        "detail": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "peekOfCode": "results = []\ndef get_region(entry, column_offset=0):\n    location = entry[\"location\"]\n    region = {\n        \"startColumn\": location[\"column\"] + column_offset,\n        \"startLine\": location[\"row\"],\n    }\n    if \"end_location\" in entry:\n        end_location = entry[\"end_location\"]\n        region[\"endColumn\"] = end_location[\"column\"] + column_offset",
        "detail": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "documentation": {}
    },
    {
        "label": "ruff_column_index",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "peekOfCode": "ruff_column_index = 1\nif len(sys.argv) > 1:\n    ruff_column_index = int(sys.argv[1])\nfor result in json.load(sys.stdin):\n    # As of ruff v0.0.260, some autofixable diagnostics may appear redundantly\n    if \"location\" not in result:\n        continue\n    filepath = result[\"filename\"]\n    rule_id = result[\"code\"]\n    message = result[\"message\"]",
        "detail": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "documentation": {}
    },
    {
        "label": "sarif",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "peekOfCode": "sarif = {\n    \"$schema\": \"https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json\",\n    \"version\": \"2.1.0\",\n    \"runs\": [{\"results\": results}],\n}\nprint(json.dumps(sarif, indent=2))",
        "detail": ".trunk.plugins.trunk.linters.ruff.ruff_to_sarif",
        "documentation": {}
    },
    {
        "label": "unvalidated_value",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.semgrep.test_data.request",
        "description": ".trunk.plugins.trunk.linters.semgrep.test_data.request",
        "peekOfCode": "def unvalidated_value(request):\n    value = request.GET.get('something')\n    function = globals().get(value)\n    if function:\n        return function(request)",
        "detail": ".trunk.plugins.trunk.linters.semgrep.test_data.request",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "peekOfCode": "def test():\n  substitution = \"hello %s\" % test\n  my_list = List()\n  try:\n    pass\n  except Exception:\n    raise Exception(\"test\")",
        "detail": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "peekOfCode": "test = \"world\"\ndef test():\n  substitution = \"hello %s\" % test\n  my_list = List()\n  try:\n    pass\n  except Exception:\n    raise Exception(\"test\")",
        "detail": ".trunk.plugins.trunk.linters.sourcery.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "peekOfCode": "def to_result_sarif(\n    path: str,\n    start_line_number: int,\n    start_column_number: int,\n    end_line_number: Optional[int],\n    end_column_number: Optional[int],\n    rule_id: str,\n    message: str,\n):\n    region = {",
        "detail": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "description": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "peekOfCode": "def main(argv):\n    sqlfluff_json = json.load(sys.stdin)\n    results = []\n    for result in sqlfluff_json:\n        filepath = result[\"filepath\"]\n        for violation in result[\"violations\"]:\n            # In sqlfluff 3.0.0, line_no/line_pos replaced with start_*/end_*\n            start_line_number = violation.get(\"start_line_no\", violation.get(\"line_no\"))\n            start_column_number = violation.get(\n                \"start_line_pos\", violation.get(\"line_pos\")",
        "detail": ".trunk.plugins.trunk.linters.sqlfluff.sqlfluff_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.terrascan.sarif_to_sarif",
        "description": ".trunk.plugins.trunk.linters.terrascan.sarif_to_sarif",
        "peekOfCode": "def main(argv):\n    input_sarif = json.load(sys.stdin)\n    # strip \"file:\" from the beginning of each value in the 'file' field in the 'location' object in sarif format\n    for run in input_sarif[\"runs\"]:\n        for result in run[\"results\"]:\n            for location in result[\"locations\"]:\n                location[\"physicalLocation\"][\"artifactLocation\"][\"uri\"] = location[\n                    \"physicalLocation\"\n                ][\"artifactLocation\"][\"uri\"][5:]\n    print(json.dumps(input_sarif, indent=2))",
        "detail": ".trunk.plugins.trunk.linters.terrascan.sarif_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.tfsec.parse",
        "description": ".trunk.plugins.trunk.linters.tfsec.parse",
        "peekOfCode": "def main():\n    original_input = sys.stdin.read()\n    try:\n        index = original_input.index(\"{\")\n        print(original_input[index:])\n    except ValueError:\n        print(original_input)\nif __name__ == \"__main__\":\n    main()",
        "detail": ".trunk.plugins.trunk.linters.tfsec.parse",
        "documentation": {}
    },
    {
        "label": "aws_access_key_id",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "aws_access_key_id = \"AKIAXYZDQCEN4EXAMPLE\"\naws_secret_access_key = \"Tg0pz8Jii8hkLx4+PnUisM8GmKs3a2DK+EXAMPLE\"\n# The below keys are copied from https://github.com/dustin-decker/secretsandstuff\ngithub_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "aws_secret_access_key",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "aws_secret_access_key = \"Tg0pz8Jii8hkLx4+PnUisM8GmKs3a2DK+EXAMPLE\"\n# The below keys are copied from https://github.com/dustin-decker/secretsandstuff\ngithub_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "github_secret",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "github_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "basic_auth",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "basic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH\nNT7eP19eVf4dCreibxUmRUaob5DEoHEk8WrxjKWIYUuLeD6AfcW6oXyRU2Yy8Vrt6SqFl5",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "priv_key",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "description": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "peekOfCode": "priv_key = \"\"\"\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH\nNT7eP19eVf4dCreibxUmRUaob5DEoHEk8WrxjKWIYUuLeD6AfcW6oXyRU2Yy8Vrt6SqFl5\nWAi47VMFTkDZYS/eCvG53q9UBHpCj7Qvb0vSkCZXBvBIhlw193F3PX4WvO1IXsMwvQ1D1X",
        "detail": ".trunk.plugins.trunk.linters.trivy.test_data.secrets",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "peekOfCode": "def to_result_sarif(path: str, vuln_id: str, description: str, line: int = 0):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "peekOfCode": "def main(argv):\n    trivy_json = json.load(sys.stdin)\n    path = trivy_json[\"ArtifactName\"]\n    results = []\n    for result in trivy_json.get(\"Results\", []):\n        if \"Misconfigurations\" not in result:\n            continue\n        for vuln in result[\"Misconfigurations\"]:\n            vuln_id = vuln[\"ID\"]\n            message = vuln[\"Message\"]",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_config_to_sarif",
        "documentation": {}
    },
    {
        "label": "get_sarif_severity",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "def get_sarif_severity(secret) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"Severity\" not in secret:\n        return DEFAULT_SARIF_SEVERITY\n    severity = secret[\"Severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(path: str, severity: str, code: str, description: str, lineno: int):\n    return {\n        \"level\": severity,\n        \"locations\": [",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "def to_result_sarif(path: str, severity: str, code: str, description: str, lineno: int):\n    return {\n        \"level\": severity,\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "def main(argv):\n    trivy_json = json.load(sys.stdin)\n    results = []\n    for result in trivy_json.get(\"Results\", []):\n        path = trivy_json[\"ArtifactName\"]\n        for secret in result.get(\"Secrets\", []):\n            code = secret[\"RuleID\"]\n            description = secret[\"Title\"]\n            lineno = secret.get(\"StartLine\", 0)\n            results.append(",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "SARIF_SEVERITY_BY_OSV_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "SARIF_SEVERITY_BY_OSV_SEVERITY = {\n    \"CRITICAL\": \"error\",\n    \"HIGH\": \"error\",\n    \"MODERATE\": \"warning\",\n    \"MEDIUM\": \"warning\",\n    \"LOW\": \"note\",\n}\nDEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(secret) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SARIF_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "peekOfCode": "DEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(secret) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"Severity\" not in secret:\n        return DEFAULT_SARIF_SEVERITY\n    severity = secret[\"Severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(path: str, severity: str, code: str, description: str, lineno: int):\n    return {\n        \"level\": severity,",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_secret_to_sarif",
        "documentation": {}
    },
    {
        "label": "get_sarif_severity",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "def get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"Severity\" not in vuln:\n        return DEFAULT_SARIF_SEVERITY\n    severity = vuln[\"Severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(\n    path: str, severity: str, vuln_id: str, description: str, lineno: int\n):\n    return {",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "def to_result_sarif(\n    path: str, severity: str, vuln_id: str, description: str, lineno: int\n):\n    return {\n        \"level\": severity,\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "def main(argv):\n    trivy_json = json.load(sys.stdin)\n    results = []\n    lockfiles = {}\n    for result in trivy_json.get(\"Results\", []):\n        for vuln in result.get(\"Vulnerabilities\", []):\n            pkg_name = vuln[\"PkgName\"]\n            path = trivy_json[\"ArtifactName\"]\n            vuln_id = vuln[\"VulnerabilityID\"]\n            description = vuln[\"Title\"]",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "SARIF_SEVERITY_BY_OSV_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "SARIF_SEVERITY_BY_OSV_SEVERITY = {\n    \"CRITICAL\": \"error\",\n    \"HIGH\": \"error\",\n    \"MODERATE\": \"warning\",\n    \"MEDIUM\": \"warning\",\n    \"LOW\": \"note\",\n}\nDEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SARIF_SEVERITY",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "peekOfCode": "DEFAULT_SARIF_SEVERITY = \"error\"\ndef get_sarif_severity(vuln) -> str:\n    \"\"\"Get the SARIF severity appropriate for a given OSV vulnerability entry.\"\"\"\n    if \"Severity\" not in vuln:\n        return DEFAULT_SARIF_SEVERITY\n    severity = vuln[\"Severity\"].upper()\n    return SARIF_SEVERITY_BY_OSV_SEVERITY.get(severity, DEFAULT_SARIF_SEVERITY)\ndef to_result_sarif(\n    path: str, severity: str, vuln_id: str, description: str, lineno: int\n):",
        "detail": ".trunk.plugins.trunk.linters.trivy.trivy_fs_vuln_to_sarif",
        "documentation": {}
    },
    {
        "label": "aws_access_key_id",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "aws_access_key_id = \"AKIAXYZDQCEN4EXAMPLE\"\naws_secret_access_key = \"Tg0pz8Jii8hkLx4+PnUisM8GmKs3a2DK+EXAMPLE\"\n# The below keys are copied from https://github.com/dustin-decker/secretsandstuff\ngithub_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "aws_secret_access_key",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "aws_secret_access_key = \"Tg0pz8Jii8hkLx4+PnUisM8GmKs3a2DK+EXAMPLE\"\n# The below keys are copied from https://github.com/dustin-decker/secretsandstuff\ngithub_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "github_secret",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "github_secret = \"369963c1434c377428ca8531fbc46c0c43d037a0\"\nbasic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "basic_auth",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "basic_auth = \"https://admin:admin@the-internet.herokuapp.com/basic_auth\"\npriv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH\nNT7eP19eVf4dCreibxUmRUaob5DEoHEk8WrxjKWIYUuLeD6AfcW6oXyRU2Yy8Vrt6SqFl5",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "priv_key",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "description": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "peekOfCode": "priv_key = '''\n-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAACmFlczI1Ni1jdHIAAAAGYmNyeXB0AAAAGAAAABAjNIZuun\nxgLkM8KuzfmQuRAAAAEAAAAAEAAAGXAAAAB3NzaC1yc2EAAAADAQABAAABgQDe3Al0EMPz\nutVNk5DixaYrGMK56RqUoqGBinke6SWVWmqom1lBcJWzor6HlnMRPPr7YCEsJKL4IpuVwu\ninRa5kdtNTyM7yyQTSR2xXCS0fUItNuq8pUktsH8VUggpMeew8hJv7rFA7tnIg3UXCl6iF\nOLZKbDA5aa24idpcD8b1I9/RzTOB1fu0of5xd9vgODzGw5JvHQSJ0FaA42aNBMGwrDhDB3\nsgnRNdWf6NNIh8KpXXMKJADf3klsyn6He8L2bPMp8a4wwys2YB35p5zQ0JURovsdewlOxH\nNT7eP19eVf4dCreibxUmRUaob5DEoHEk8WrxjKWIYUuLeD6AfcW6oXyRU2Yy8Vrt6SqFl5\nWAi47VMFTkDZYS/eCvG53q9UBHpCj7Qvb0vSkCZXBvBIhlw193F3PX4WvO1IXsMwvQ1D1X",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.test_data.secrets.in",
        "documentation": {}
    },
    {
        "label": "to_result_sarif",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "def to_result_sarif(path: str, line_number: int, vuln_id: str, description: str):\n    return {\n        \"level\": \"error\",\n        \"locations\": [\n            {\n                \"physicalLocation\": {\n                    \"artifactLocation\": {\n                        \"uri\": path,\n                    },\n                    \"region\": {",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "def sliding_window(iterable, n):\n    # sliding_window('ABCDEFG', 4) --> ABCD BCDE CDEF DEFG\n    it = iter(iterable)\n    window = collections.deque(islice(it, n - 1), maxlen=n)\n    for x in it:\n        window.append(x)\n        yield tuple(window)\nsecret_lineno_cache = {}\nfile_cache = {}\ndef find_line_number(secret, path):",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "find_line_number",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "def find_line_number(secret, path):\n    if path not in file_cache:\n        file_cache[path] = open(path).readlines()\n    if secret not in secret_lineno_cache:\n        secret_lineno_cache[secret] = []\n    secret_length = len(secret.splitlines())\n    lines = file_cache[path]\n    for lineno, window in enumerate(sliding_window(lines, secret_length), 1):\n        # trufflehog can report the same secret multiple times\n        # if it truly appears multiple times, then we want to log different lines for each issue",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "def main(argv):\n    results = []\n    for line in sys.stdin.readlines():\n        vuln_json = json.loads(line)\n        # trufflehog doesn't have vuln IDs\n        # this is the name of the detector that found the error (e.g. AWS, Github, PrivateKey)\n        vuln_id = vuln_json[\"DetectorName\"]\n        # There also isn't description of the error aside from the raw secret, the redacted secret,\n        # and the detector that found it.\n        #",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "secret_lineno_cache",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "secret_lineno_cache = {}\nfile_cache = {}\ndef find_line_number(secret, path):\n    if path not in file_cache:\n        file_cache[path] = open(path).readlines()\n    if secret not in secret_lineno_cache:\n        secret_lineno_cache[secret] = []\n    secret_length = len(secret.splitlines())\n    lines = file_cache[path]\n    for lineno, window in enumerate(sliding_window(lines, secret_length), 1):",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "file_cache",
        "kind": 5,
        "importPath": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "description": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "peekOfCode": "file_cache = {}\ndef find_line_number(secret, path):\n    if path not in file_cache:\n        file_cache[path] = open(path).readlines()\n    if secret not in secret_lineno_cache:\n        secret_lineno_cache[secret] = []\n    secret_length = len(secret.splitlines())\n    lines = file_cache[path]\n    for lineno, window in enumerate(sliding_window(lines, secret_length), 1):\n        # trufflehog can report the same secret multiple times",
        "detail": ".trunk.plugins.trunk.linters.trufflehog.trufflehog_to_sarif",
        "documentation": {}
    },
    {
        "label": "NoDocstring",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "peekOfCode": "class NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1\nclass Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n#whitespace below vvv\n  #A malindented comment\nif __name__ == \"__main__\" :\n      a=4+1",
        "detail": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "Globe",
        "kind": 6,
        "importPath": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "peekOfCode": "class Globe(object):\n    def __init__(self):\n        self.shape = 'spheroid'\n#whitespace below vvv\n  #A malindented comment\nif __name__ == \"__main__\" :\n      a=4+1\n      b=( 2*7 )\n      c = [1,\n           2,",
        "detail": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "description": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "peekOfCode": "def main():\n    try:\n        pass\n    except (Exception, TypeError):\n        pass\nimport sys\n# trunk-ignore(flake8/F401): this will trigger a warning to verify that the config is applied\nclass NoDocstring(object):\n    def __init__(self, arg1):\n        self._attr1 = arg1",
        "detail": ".trunk.plugins.trunk.linters.yapf.test_data.basic.in",
        "documentation": {}
    },
    {
        "label": "scrub",
        "kind": 2,
        "importPath": "mattmc3.zman.bin.zmandata",
        "description": "mattmc3.zman.bin.zmandata",
        "peekOfCode": "def scrub(s):\n    s = s.replace(endash, \"--\")\n    s = s.replace(emdash, \"--\")\n    return s\ndef process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:\n        for row in tbl(\"tr\"):\n            tds = row(\"td\")\n            # example:",
        "detail": "mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "process_tables",
        "kind": 2,
        "importPath": "mattmc3.zman.bin.zmandata",
        "description": "mattmc3.zman.bin.zmandata",
        "peekOfCode": "def process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:\n        for row in tbl(\"tr\"):\n            tds = row(\"td\")\n            # example:\n            # [<td></td>,\n            # <td valign=\"top\"><a href=\"Options.html#index-C_005fPRECEDENCES\"><code>C_PRECEDENCES</code></a></td>,\n            # <td> </td>,\n            # <td valign=\"top\"><a href=\"Options.html#Scripts-and-Functions\">16.2.9 Scripts and Functions</a></td>]",
        "detail": "mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "save_zdoc_entries",
        "kind": 2,
        "importPath": "mattmc3.zman.bin.zmandata",
        "description": "mattmc3.zman.bin.zmandata",
        "peekOfCode": "def save_zdoc_entries(prjdir, zdoc_entries):\n    zdoc_entries_sorted = sorted(zdoc_entries, key=lambda x: x.phrase.lower())\n    zman_data_path = prjdir / \"docs\" / \"zmandata.tsv\"\n    print(f\"writing {zman_data_path.name}\")\n    with zman_data_path.open('w') as f:\n        f.write(\"\\t\".join(IndexEntry._fields) + \"\\n\")\n        for e in zdoc_entries_sorted:\n            f.write(\"\\t\".join(e) + \"\\n\")\n    zman_dict = {}\n    for e in zdoc_entries_sorted:",
        "detail": "mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "mattmc3.zman.bin.zmandata",
        "description": "mattmc3.zman.bin.zmandata",
        "peekOfCode": "def main():\n    prjdir = Path(__file__).absolute().parent.parent\n    zmanindex_classes = {\n        \"concepts\": \"index-cp\",\n        \"variables\": \"index-vr\",\n        \"options\": \"index-pg\",\n        \"functions\": \"index-fn\",\n        \"editor-functions\": \"index-tp\",\n        \"styles-and-tags\": \"index-ky\",\n    }",
        "detail": "mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "IndexEntry",
        "kind": 5,
        "importPath": "mattmc3.zman.bin.zmandata",
        "description": "mattmc3.zman.bin.zmandata",
        "peekOfCode": "IndexEntry = namedtuple(\"IndexEntry\", \"phrase category indexed_at url section section_url\")\nendash = '\\u2013'\nemdash = '\\u2014'\ndef scrub(s):\n    s = s.replace(endash, \"--\")\n    s = s.replace(emdash, \"--\")\n    return s\ndef process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:",
        "detail": "mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "endash",
        "kind": 5,
        "importPath": "mattmc3.zman.bin.zmandata",
        "description": "mattmc3.zman.bin.zmandata",
        "peekOfCode": "endash = '\\u2013'\nemdash = '\\u2014'\ndef scrub(s):\n    s = s.replace(endash, \"--\")\n    s = s.replace(emdash, \"--\")\n    return s\ndef process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:\n        for row in tbl(\"tr\"):",
        "detail": "mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "emdash",
        "kind": 5,
        "importPath": "mattmc3.zman.bin.zmandata",
        "description": "mattmc3.zman.bin.zmandata",
        "peekOfCode": "emdash = '\\u2014'\ndef scrub(s):\n    s = s.replace(endash, \"--\")\n    s = s.replace(emdash, \"--\")\n    return s\ndef process_tables(tbls, indexed_at, category):\n    entries = []\n    for tbl in tbls:\n        for row in tbl(\"tr\"):\n            tds = row(\"td\")",
        "detail": "mattmc3.zman.bin.zmandata",
        "documentation": {}
    },
    {
        "label": "CodeTimer",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class CodeTimer:\n    def __init__(self, name=None):\n        self.name = \" '\" + name + \"'\" if name else \"\"\n    def __enter__(self):\n        self.start = timeit.default_timer()\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.took = (timeit.default_timer() - self.start) * 1000.0\n        print(\"Code block\" + self.name + \" took: \" + str(self.took) + \" ms\")\n### YAML representation\ndef str_presenter(dumper, data):",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DependencyDict",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class DependencyDict(TypedDict):\n    repo: str\n    branch: str\n    version: str\n    precopy: NotRequired[str]\n    postcopy: NotRequired[str]\nclass DependencyYAML(TypedDict):\n    dependencies: dict[str, DependencyDict]\nclass UpdateStatusFalse(TypedDict):\n    has_updates: Literal[False]",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DependencyYAML",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class DependencyYAML(TypedDict):\n    dependencies: dict[str, DependencyDict]\nclass UpdateStatusFalse(TypedDict):\n    has_updates: Literal[False]\nclass UpdateStatusTrue(TypedDict):\n    has_updates: Literal[True]\n    version: str\n    compare_url: str\n    head_ref: str\n    head_url: str",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "UpdateStatusFalse",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class UpdateStatusFalse(TypedDict):\n    has_updates: Literal[False]\nclass UpdateStatusTrue(TypedDict):\n    has_updates: Literal[True]\n    version: str\n    compare_url: str\n    head_ref: str\n    head_url: str\nclass CommandRunner:\n    class Exception(Exception):",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "UpdateStatusTrue",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class UpdateStatusTrue(TypedDict):\n    has_updates: Literal[True]\n    version: str\n    compare_url: str\n    head_ref: str\n    head_url: str\nclass CommandRunner:\n    class Exception(Exception):\n        def __init__(self, message, returncode, stage, stdout, stderr):\n            super().__init__(message)",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "CommandRunner",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class CommandRunner:\n    class Exception(Exception):\n        def __init__(self, message, returncode, stage, stdout, stderr):\n            super().__init__(message)\n            self.returncode = returncode\n            self.stage = stage\n            self.stdout = stdout\n            self.stderr = stderr\n    @staticmethod\n    def run_or_fail(command: list[str], stage: str, *args, **kwargs):",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DependencyStore",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class DependencyStore:\n    store: DependencyYAML = {\"dependencies\": {}}\n    @staticmethod\n    def set(data: DependencyYAML):\n        DependencyStore.store = data\n    @staticmethod\n    def update_dependency_version(path: str, version: str) -> DependencyYAML:\n        with CodeTimer(f\"store deepcopy: {path}\"):\n            store_copy = deepcopy(DependencyStore.store)\n        dependency = store_copy[\"dependencies\"].get(path)",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "Dependency",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class Dependency:\n    def __init__(self, path: str, values: DependencyDict):\n        self.path = path\n        self.values = values\n        self.name: str = \"\"\n        self.desc: str = \"\"\n        self.kind: str = \"\"\n        match path.split(\"/\"):\n            case [\"plugins\", name]:\n                self.name = name",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "Git",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class Git:\n    default_branch = \"master\"\n    @staticmethod\n    def clone(remote_url: str, branch: str, repo_dir: str, reclone=False):\n        # If repo needs to be fresh\n        if reclone and os.path.exists(repo_dir):\n            shutil.rmtree(repo_dir)\n        # Clone repo in tmp directory and checkout branch\n        if not os.path.exists(repo_dir):\n            print(",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "GitHub",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "class GitHub:\n    @staticmethod\n    def check_newer_tag(repo, current_tag) -> UpdateStatusFalse | UpdateStatusTrue:\n        # GET /repos/:owner/:repo/git/refs/tags\n        url = f\"https://api.github.com/repos/{repo}/git/refs/tags\"\n        # Send a GET request to the GitHub API\n        response = requests.get(url)\n        current_version = coerce(current_tag)\n        if current_version is None:\n            raise ValueError(",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "coerce",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "def coerce(version: str) -> Optional[Version]:\n    match = BASEVERSION.search(version)\n    if not match:\n        return None\n    # BASEVERSION looks for `MAJOR.minor.patch` in the string given\n    # it fills with None if any of them is missing (for example `2.1`)\n    ver = {\n        key: 0 if value is None else value for key, value in match.groupdict().items()\n    }\n    # Version takes `major`, `minor`, `patch` arguments",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "str_presenter",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "def str_presenter(dumper, data):\n    \"\"\"\n    Configures yaml for dumping multiline strings\n    Ref: https://stackoverflow.com/a/33300001\n    \"\"\"\n    if len(data.splitlines()) > 1:  # check for multiline string\n        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"|\")\n    return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data)\nyaml.add_representer(str, str_presenter)\nyaml.representer.SafeRepresenter.add_representer(str, str_presenter)",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "def main():\n    # Load the YAML file\n    with open(DEPS_YAML_FILE, \"r\") as yaml_file:\n        data: DependencyYAML = yaml.safe_load(yaml_file)\n    if \"dependencies\" not in data:\n        raise Exception(\"dependencies.yml not properly formatted\")\n    # Cache YAML version\n    DependencyStore.set(data)\n    dependencies = data[\"dependencies\"]\n    for path in dependencies:",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "TMP_DIR",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "TMP_DIR = os.path.join(os.environ.get(\"TMP_DIR\", \"/tmp\"), \"ohmyzsh\")\n# Relative path to dependencies.yml file\nDEPS_YAML_FILE = \".github/dependencies.yml\"\n# Dry run flag\nDRY_RUN = os.environ.get(\"DRY_RUN\", \"0\") == \"1\"\n# utils for tag comparison\nBASEVERSION = re.compile(\n    r\"\"\"[vV]?\n    (?P<major>(0|[1-9])\\d*)\n    (\\.",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DEPS_YAML_FILE",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "DEPS_YAML_FILE = \".github/dependencies.yml\"\n# Dry run flag\nDRY_RUN = os.environ.get(\"DRY_RUN\", \"0\") == \"1\"\n# utils for tag comparison\nBASEVERSION = re.compile(\n    r\"\"\"[vV]?\n    (?P<major>(0|[1-9])\\d*)\n    (\\.\n    (?P<minor>(0|[1-9])\\d*)\n    (\\.",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "DRY_RUN",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "DRY_RUN = os.environ.get(\"DRY_RUN\", \"0\") == \"1\"\n# utils for tag comparison\nBASEVERSION = re.compile(\n    r\"\"\"[vV]?\n    (?P<major>(0|[1-9])\\d*)\n    (\\.\n    (?P<minor>(0|[1-9])\\d*)\n    (\\.\n    (?P<patch>(0|[1-9])\\d*)\n    )?",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "BASEVERSION",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "description": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "peekOfCode": "BASEVERSION = re.compile(\n    r\"\"\"[vV]?\n    (?P<major>(0|[1-9])\\d*)\n    (\\.\n    (?P<minor>(0|[1-9])\\d*)\n    (\\.\n    (?P<patch>(0|[1-9])\\d*)\n    )?\n    )?\n    \"\"\",",
        "detail": "ohmyzsh.ohmyzsh..github.workflows.dependencies.updater",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "peekOfCode": "def parse(line):\n    left = line[0:line.find('=')].strip()\n    right = line[line.find('=')+1:].strip('\\'\"\\n ')\n    try:\n        cmd = next(part for part in right.split() if len([char for char in '=<>' if char in part])==0)\n    except StopIteration:\n        cmd = right\n    return (left, right, cmd)\ndef cheatsheet(lines):\n    exps = [ parse(line) for line in lines ]",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "documentation": {}
    },
    {
        "label": "cheatsheet",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "peekOfCode": "def cheatsheet(lines):\n    exps = [ parse(line) for line in lines ]\n    exps.sort(key=lambda exp:exp[2])\n    cheatsheet = {'_default': []}\n    for key, group in itertools.groupby(exps, lambda exp:exp[2]):\n        group_list = [ item for item in group ]\n        if len(group_list)==1:\n            target_aliases = cheatsheet['_default']\n        else:\n            if key not in cheatsheet:",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "documentation": {}
    },
    {
        "label": "pretty_print_group",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "peekOfCode": "def pretty_print_group(key, aliases, highlight=None, only_groupname=False):\n    if len(aliases) == 0:\n        return\n    group_hl_formatter = lambda g, hl: termcolor.colored(hl, 'yellow').join([termcolor.colored(part, 'red') for part in ('[%s]' % g).split(hl)])\n    alias_hl_formatter = lambda alias, hl: termcolor.colored(hl, 'yellow').join([termcolor.colored(part, 'green') for part in ('\\t%s = %s' % alias[0:2]).split(hl)])\n    group_formatter = lambda g: termcolor.colored('[%s]' % g, 'red')\n    alias_formatter = lambda alias: termcolor.colored('\\t%s = %s' % alias[0:2], 'green')\n    if highlight and len(highlight)>0:\n        print (group_hl_formatter(key, highlight))\n        if not only_groupname:",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "documentation": {}
    },
    {
        "label": "pretty_print",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "peekOfCode": "def pretty_print(cheatsheet, wfilter, group_list=None, groups_only=False):\n    sorted_key = sorted(cheatsheet.keys())\n    for key in sorted_key:\n        if group_list and key not in group_list:\n            continue\n        aliases = cheatsheet.get(key)\n        if not wfilter:\n            pretty_print_group(key, aliases, wfilter, groups_only)\n        else:\n            pretty_print_group(key, [ alias for alias in aliases if alias[0].find(wfilter)>-1 or alias[1].find(wfilter)>-1], wfilter)",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.cheatsheet",
        "documentation": {}
    },
    {
        "label": "colored",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "def colored(text, color=None, on_color=None, attrs=None):\n    \"\"\"Colorize text.\n    Available text colors:\n        red, green, yellow, blue, magenta, cyan, white.\n    Available text highlights:\n        on_red, on_green, on_yellow, on_blue, on_magenta, on_cyan, on_white.\n    Available attributes:\n        bold, dark, underline, blink, reverse, concealed.\n    Example:\n        colored('Hello, World!', 'red', 'on_grey', ['blue', 'blink'])",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "cprint",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "def cprint(text, color=None, on_color=None, attrs=None, **kwargs):\n    \"\"\"Print colorize text.\n    It accepts arguments of print function.\n    \"\"\"\n    print((colored(text, color, on_color, attrs)), **kwargs)\nif __name__ == '__main__':\n    print('Current terminal type: %s' % os.getenv('TERM'))\n    print('Test basic colors:')\n    cprint('Grey color', 'grey')\n    cprint('Red color', 'red')",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "__ALL__",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "__ALL__ = [ 'colored', 'cprint' ]\nVERSION = (1, 1, 0)\nATTRIBUTES = dict(\n        list(zip([\n            'bold',\n            'dark',\n            '',\n            'underline',\n            'blink',\n            '',",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "VERSION",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "VERSION = (1, 1, 0)\nATTRIBUTES = dict(\n        list(zip([\n            'bold',\n            'dark',\n            '',\n            'underline',\n            'blink',\n            '',\n            'reverse',",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "ATTRIBUTES",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "ATTRIBUTES = dict(\n        list(zip([\n            'bold',\n            'dark',\n            '',\n            'underline',\n            'blink',\n            '',\n            'reverse',\n            'concealed'",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "HIGHLIGHTS",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "HIGHLIGHTS = dict(\n        list(zip([\n            'on_grey',\n            'on_red',\n            'on_green',\n            'on_yellow',\n            'on_blue',\n            'on_magenta',\n            'on_cyan',\n            'on_white'",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "COLORS",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "COLORS = dict(\n        list(zip([\n            'grey',\n            'red',\n            'green',\n            'yellow',\n            'blue',\n            'magenta',\n            'cyan',\n            'white',",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "RESET",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "description": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "peekOfCode": "RESET = '\\033[0m'\ndef colored(text, color=None, on_color=None, attrs=None):\n    \"\"\"Colorize text.\n    Available text colors:\n        red, green, yellow, blue, magenta, cyan, white.\n    Available text highlights:\n        on_red, on_green, on_yellow, on_blue, on_magenta, on_cyan, on_white.\n    Available attributes:\n        bold, dark, underline, blink, reverse, concealed.\n    Example:",
        "detail": "ohmyzsh.ohmyzsh.plugins.aliases.termcolor",
        "documentation": {}
    },
    {
        "label": "code_to_omz",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "def code_to_omz(_code_points):\n    \"\"\" Returns a ZSH-compatible Unicode string from the code point(s) \"\"\"\n    return r'\\U' + r'\\U'.join(_code_points.split(' '))\ndef name_to_omz(_name, _group, _subgroup, _status):\n    \"\"\" Returns a reasonable snake_case name for the emoji. \"\"\"\n    def snake_case(_string):\n        \"\"\" Does the regex work of snake_case \"\"\"\n        remove_dots = re.sub(r'\\.\\(\\)', r'', _string)\n        replace_ands = re.sub(r'\\&', r'and', remove_dots)\n        remove_whitespace = re.sub(r'[^\\#\\*\\w]', r'_', replace_ands)",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "name_to_omz",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "def name_to_omz(_name, _group, _subgroup, _status):\n    \"\"\" Returns a reasonable snake_case name for the emoji. \"\"\"\n    def snake_case(_string):\n        \"\"\" Does the regex work of snake_case \"\"\"\n        remove_dots = re.sub(r'\\.\\(\\)', r'', _string)\n        replace_ands = re.sub(r'\\&', r'and', remove_dots)\n        remove_whitespace = re.sub(r'[^\\#\\*\\w]', r'_', replace_ands)\n        return re.sub(r'__', r'_', remove_whitespace)\n    shortname = \"\"\n    split_at_colon = lambda s: s.split(\": \")",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "increment_name",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "def increment_name(_shortname):\n    \"\"\" Increment the short name by 1. If you get, say,\n    'woman_detective_unqualified', it returns\n    'woman_detective_unqualified_1', and then\n    'woman_detective_unqualified_2', etc. \"\"\"\n    last_char = _shortname[-1]\n    if last_char.isdigit():\n        num = int(last_char)\n        return _shortname[:-1] + str(num + 1)\n    return _shortname + \"_1\"",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "spec",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "spec = open(\"emoji-data.txt\", \"r\")\n# Regexes\n# regex_emoji will return, respectively:\n# the code points, its type (status), the actual emoji, and its official name\nregex_emoji = r\"^([\\w ].*?\\S)\\s*;\\s*([\\w-]+)\\s*#\\s*(.*?)\\s(\\S.*).*$\"\n# regex_group returns the group of subgroup that a line opens\nregex_group = r\"^#\\s*(group|subgroup):\\s*(.*)$\"\nheaders = \"\"\"\n# emoji-char-definitions.zsh - Emoji definitions for oh-my-zsh emoji plugin\n#",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "regex_emoji",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "regex_emoji = r\"^([\\w ].*?\\S)\\s*;\\s*([\\w-]+)\\s*#\\s*(.*?)\\s(\\S.*).*$\"\n# regex_group returns the group of subgroup that a line opens\nregex_group = r\"^#\\s*(group|subgroup):\\s*(.*)$\"\nheaders = \"\"\"\n# emoji-char-definitions.zsh - Emoji definitions for oh-my-zsh emoji plugin\n#\n# This file is auto-generated by update_emoji.py. Do not edit it manually.\n#\n# This contains the definition for:\n#   $emoji         - which maps character names to Unicode characters",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "regex_group",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "regex_group = r\"^#\\s*(group|subgroup):\\s*(.*)$\"\nheaders = \"\"\"\n# emoji-char-definitions.zsh - Emoji definitions for oh-my-zsh emoji plugin\n#\n# This file is auto-generated by update_emoji.py. Do not edit it manually.\n#\n# This contains the definition for:\n#   $emoji         - which maps character names to Unicode characters\n#   $emoji_flags   - maps country names to Unicode flag characters using region\n#                    indicators",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "headers",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "headers = \"\"\"\n# emoji-char-definitions.zsh - Emoji definitions for oh-my-zsh emoji plugin\n#\n# This file is auto-generated by update_emoji.py. Do not edit it manually.\n#\n# This contains the definition for:\n#   $emoji         - which maps character names to Unicode characters\n#   $emoji_flags   - maps country names to Unicode flag characters using region\n#                    indicators\n#   $emoji_mod     - maps modifier components to Unicode characters",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "emoji_database",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "emoji_database = []\nfor line in spec:\n    # First, test if this line opens a group or subgroup\n    group_match = re.findall(regex_group, line)\n    if group_match != []:\n        gr_or_sub, name = group_match[0]\n        if gr_or_sub == \"group\":\n            group = name\n        elif gr_or_sub == \"subgroup\":\n            subgroup = name",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "gemoji_db",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "gemoji_db = open(\"gemoji_db.json\")\nj = json.load(gemoji_db)\naliases_map = {entry['emoji']: entry['aliases'] for entry in j}\nall_omz_names = [emoji_data[3] for emoji_data in emoji_database]\n# Let's begin writing to this file\noutput = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "j",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "j = json.load(gemoji_db)\naliases_map = {entry['emoji']: entry['aliases'] for entry in j}\nall_omz_names = [emoji_data[3] for emoji_data in emoji_database]\n# Let's begin writing to this file\noutput = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "aliases_map",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "aliases_map = {entry['emoji']: entry['aliases'] for entry in j}\nall_omz_names = [emoji_data[3] for emoji_data in emoji_database]\n# Let's begin writing to this file\noutput = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down\nfor _omz_codes, _status, _emoji, _omz_name, _group, _subgroup in emoji_database:",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "all_omz_names",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "all_omz_names = [emoji_data[3] for emoji_data in emoji_database]\n# Let's begin writing to this file\noutput = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down\nfor _omz_codes, _status, _emoji, _omz_name, _group, _subgroup in emoji_database:\n    # One emoji can be mapped to multiple names (aliases or country codes)",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "output = open(\"emoji-char-definitions.zsh\", \"w\")\noutput.write(headers)\nemoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down\nfor _omz_codes, _status, _emoji, _omz_name, _group, _subgroup in emoji_database:\n    # One emoji can be mapped to multiple names (aliases or country codes)\n    names_for_this_emoji = [_omz_name]\n    # Variable that indicates in which map the emoji will be located",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "emoji_groups",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "description": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "peekOfCode": "emoji_groups = {\"fruits\": \"\\n\", \"vehicles\": \"\\n\", \"hands\": \"\\n\",\n                \"people\": \"\\n\", \"animals\": \"\\n\", \"faces\": \"\\n\",\n                \"flags\": \"\\n\"}\n# First, write every emoji down\nfor _omz_codes, _status, _emoji, _omz_name, _group, _subgroup in emoji_database:\n    # One emoji can be mapped to multiple names (aliases or country codes)\n    names_for_this_emoji = [_omz_name]\n    # Variable that indicates in which map the emoji will be located\n    emoji_map = \"emoji\"\n    if _status == \"component\":",
        "detail": "ohmyzsh.ohmyzsh.plugins.emoji.update_emoji",
        "documentation": {}
    },
    {
        "label": "get_tagname_or_hash",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "def get_tagname_or_hash():\n    \"\"\"return tagname if exists else hash\"\"\"\n    # get hash\n    hash_cmd = ['git', 'rev-parse', '--short', 'HEAD']\n    hash_ = check_output(hash_cmd).decode('utf-8').strip()\n    # get tagname\n    tags_cmd = ['git', 'for-each-ref', '--points-at=HEAD', '--count=2', '--sort=-version:refname', '--format=%(refname:short)', 'refs/tags']\n    tags = check_output(tags_cmd).decode('utf-8').split()\n    if tags:\n        return tags[0] + ('+' if len(tags) > 1 else '')",
        "detail": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "get_stash",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "def get_stash():\n    cmd = Popen(['git', 'rev-parse', '--git-dir'], stdout=PIPE, stderr=PIPE)\n    so, se = cmd.communicate()\n    stash_file = '%s%s' % (so.decode('utf-8').rstrip(), '/logs/refs/stash')\n    try:\n        with open(stash_file) as f:\n            return sum(1 for _ in f)\n    except IOError:\n        return 0\n# `git status --porcelain --branch` can collect all information",
        "detail": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "po",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "po = Popen(['git', 'status', '--porcelain', '--branch'], env=dict(os.environ, LANG=\"C\"), stdout=PIPE, stderr=PIPE)\nstdout, sterr = po.communicate()\nif po.returncode != 0:\n    sys.exit(0)  # Not a git repository\n# collect git status information\nuntracked, staged, changed, deleted, conflicts = [], [], [], [], []\nahead, behind = 0, 0\nstatus = [(line[0], line[1], line[2:]) for line in stdout.decode('utf-8').splitlines()]\nfor st in status:\n    if st[0] == '#' and st[1] == '#':",
        "detail": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "status",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "status = [(line[0], line[1], line[2:]) for line in stdout.decode('utf-8').splitlines()]\nfor st in status:\n    if st[0] == '#' and st[1] == '#':\n        if re.search('Initial commit on', st[2]) or re.search('No commits yet on', st[2]):\n            branch = st[2].split(' ')[-1]\n        elif re.search('no branch', st[2]):  # detached status\n            branch = get_tagname_or_hash()\n        elif len(st[2].strip().split('...')) == 1:\n            branch = st[2].strip()\n        else:",
        "detail": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "stashed",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "stashed = get_stash()\nif not changed and not deleted and not staged and not conflicts and not untracked:\n    clean = 1\nelse:\n    clean = 0\nout = ' '.join([\n    branch,\n    str(ahead),\n    str(behind),\n    str(len(staged)),",
        "detail": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "out",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "description": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "peekOfCode": "out = ' '.join([\n    branch,\n    str(ahead),\n    str(behind),\n    str(len(staged)),\n    str(len(conflicts)),\n    str(len(changed)),\n    str(len(untracked)),\n    str(stashed),\n    str(clean),",
        "detail": "ohmyzsh.ohmyzsh.plugins.git-prompt.gitstatus",
        "documentation": {}
    },
    {
        "label": "CommandSet",
        "kind": 6,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "class CommandSet:\n    proxies = make_proxies(*get_http_proxy())\n    aliases = {\n        _: \"env __SSH_PROGRAM_NAME__=%s %s\" % (_, ssh_agent)\n        for _ in (\"ssh\", \"sftp\", \"scp\", \"slogin\", \"ssh-copy-id\")\n    }\n    def enable(self):\n        cmdline(\"export\", *merge(self.proxies))\n        cmdline(\"alias\", *merge(self.aliases))\n    def disable(self):",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "get_http_proxy",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)\n    if default_proxy and no_proxy:\n        return default_proxy, no_proxy\n    if os.path.isfile(proxy_config):\n        proxy_configdata = [line.strip() for line in check_output(proxy_config).decode(\"utf-8\").splitlines()]\n        if len(proxy_configdata) >= 1:\n            if not default_proxy:\n                default_proxy = proxy_configdata[0]",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "make_proxies",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def make_proxies(url: str, no_proxy: str):\n    proxies = {\"%s_PROXY\" % _: url for _ in (\"HTTP\", \"HTTPS\", \"FTP\", \"RSYNC\", \"ALL\")}\n    proxies.update({name.lower(): value for (name, value) in proxies.items()})\n    proxies[\"GIT_SSH\"] = ssh_agent\n    if no_proxy:\n        proxies.update({\"NO_PROXY\": no_proxy, \"no_proxy\": no_proxy})\n    return proxies\ndef merge(mapping: dict):\n    return (\"%s=%s\" % _ for _ in mapping.items())\nclass CommandSet:",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "merge",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def merge(mapping: dict):\n    return (\"%s=%s\" % _ for _ in mapping.items())\nclass CommandSet:\n    proxies = make_proxies(*get_http_proxy())\n    aliases = {\n        _: \"env __SSH_PROGRAM_NAME__=%s %s\" % (_, ssh_agent)\n        for _ in (\"ssh\", \"sftp\", \"scp\", \"slogin\", \"ssh-copy-id\")\n    }\n    def enable(self):\n        cmdline(\"export\", *merge(self.proxies))",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "cmdline",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def cmdline(*items):\n    print(list2cmdline(items))\ndef main():\n    command = CommandSet()\n    if len(sys.argv) == 1:\n        command.usage()\n        sys.exit(1)\n    getattr(command, sys.argv[1], command.usage)()\nif __name__ == \"__main__\":\n    main()",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "def main():\n    command = CommandSet()\n    if len(sys.argv) == 1:\n        command.usage()\n        sys.exit(1)\n    getattr(command, sys.argv[1], command.usage)()\nif __name__ == \"__main__\":\n    main()",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "cwd",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "cwd = os.path.dirname(__file__)\nssh_agent = os.path.join(cwd, \"ssh-agent.py\")\nproxy_env = \"SHELLPROXY_URL\"\nno_proxy_env = \"SHELLPROXY_NO_PROXY\"\nproxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "ssh_agent",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "ssh_agent = os.path.join(cwd, \"ssh-agent.py\")\nproxy_env = \"SHELLPROXY_URL\"\nno_proxy_env = \"SHELLPROXY_NO_PROXY\"\nproxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "proxy_env",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "proxy_env = \"SHELLPROXY_URL\"\nno_proxy_env = \"SHELLPROXY_NO_PROXY\"\nproxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)\n    if default_proxy and no_proxy:",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "no_proxy_env",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "no_proxy_env = \"SHELLPROXY_NO_PROXY\"\nproxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)\n    if default_proxy and no_proxy:\n        return default_proxy, no_proxy",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "proxy_config",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "peekOfCode": "proxy_config = os.environ.get(\"SHELLPROXY_CONFIG\") or os.path.expandvars(\"$HOME/.config/proxy\")\nusage=\"\"\"shell-proxy: no proxy configuration found.\nSet `{env}` or create a config file at `{config}`\nSee the plugin README for more information.\"\"\".format(env=proxy_env, config=proxy_config)\ndef get_http_proxy():\n    default_proxy = os.environ.get(proxy_env)\n    no_proxy = os.environ.get(no_proxy_env)\n    if default_proxy and no_proxy:\n        return default_proxy, no_proxy\n    if os.path.isfile(proxy_config):",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.proxy",
        "documentation": {}
    },
    {
        "label": "ssh_proxy",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "peekOfCode": "ssh_proxy = os.path.join(os.path.dirname(__file__), \"ssh-proxy.py\")\nargv = [\n    os.environ.get(\"__SSH_PROGRAM_NAME__\", \"ssh\"),\n    \"-o\",\n    \"ProxyCommand={} %h %p\".format(ssh_proxy),\n    \"-o\",\n    \"Compression=yes\",\n]\nsubprocess.call(argv + sys.argv[1:], env=os.environ)",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "documentation": {}
    },
    {
        "label": "argv",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "peekOfCode": "argv = [\n    os.environ.get(\"__SSH_PROGRAM_NAME__\", \"ssh\"),\n    \"-o\",\n    \"ProxyCommand={} %h %p\".format(ssh_proxy),\n    \"-o\",\n    \"Compression=yes\",\n]\nsubprocess.call(argv + sys.argv[1:], env=os.environ)",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-agent",
        "documentation": {}
    },
    {
        "label": "make_argv",
        "kind": 2,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "peekOfCode": "def make_argv():\n    yield \"nc\"\n    if sys.platform in {'linux', 'cygwin'}:\n        # caveats: the built-in netcat of most linux distributions and cygwin support proxy type\n        # caveats: macOS built-in netcat command not supported proxy-type\n        yield \"-X\" # --proxy-type\n        # Supported protocols are 4 (SOCKS v4), 5 (SOCKS v5) and connect (HTTP proxy).\n        # Default SOCKS v5 is used.\n        yield proxy_protocols[parsed.scheme]\n    yield \"-x\" # --proxy",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "documentation": {}
    },
    {
        "label": "proxy",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "peekOfCode": "proxy = next(os.environ[_] for _ in (\"HTTP_PROXY\", \"HTTPS_PROXY\") if _ in os.environ)\nparsed = urlparse(proxy)\nproxy_protocols = {\n    \"http\": \"connect\",\n    \"https\": \"connect\",\n    \"socks\": \"5\",\n    \"socks5\": \"5\",\n    \"socks4\": \"4\",\n    \"socks4a\": \"4\",\n}",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "documentation": {}
    },
    {
        "label": "parsed",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "peekOfCode": "parsed = urlparse(proxy)\nproxy_protocols = {\n    \"http\": \"connect\",\n    \"https\": \"connect\",\n    \"socks\": \"5\",\n    \"socks5\": \"5\",\n    \"socks4\": \"4\",\n    \"socks4a\": \"4\",\n}\nif parsed.scheme not in proxy_protocols:",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "documentation": {}
    },
    {
        "label": "proxy_protocols",
        "kind": 5,
        "importPath": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "description": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "peekOfCode": "proxy_protocols = {\n    \"http\": \"connect\",\n    \"https\": \"connect\",\n    \"socks\": \"5\",\n    \"socks5\": \"5\",\n    \"socks4\": \"4\",\n    \"socks4a\": \"4\",\n}\nif parsed.scheme not in proxy_protocols:\n    raise TypeError('unsupported proxy protocol: \"{}\"'.format(parsed.scheme))",
        "detail": "ohmyzsh.ohmyzsh.plugins.shell-proxy.ssh-proxy",
        "documentation": {}
    }
]